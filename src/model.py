# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TeyfBNB05G7XG1jaMoQKyMiuaelX1kvn
"""

# Importing libraries for Data Manipulation
import pandas as pd
import numpy as np

# Importing libraries for Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Importing libraries for Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Importing libraries for Deep Learning
import torch
import torch.nn as nn
import torch.optim as optim

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Colab_Notebooks/cleaned_dataset.csv'

df = pd.read_csv(file_path)

# Drop the 'Date' column
df = df.drop(columns=['Date'])

# to predict the 'Close' price
X = df.drop(columns=['Close'])   #can't use as an input
y = df['Close']  # Target variable

# Split the data (80% for training, 20% for testing)
train_size = int(len(df) * 0.8)

# Training and testing data (for time series, do not shuffle)
X_train = X.iloc[:train_size]
y_train = y.iloc[:train_size]
X_test = X.iloc[train_size:]
y_test = y.iloc[train_size:]

# Print the shape of the split data
print(f"Training Set Shape: X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"Testing Set Shape: X_test: {X_test.shape}, y_test: {y_test.shape}")

# Reshaping the data for LSTM/GRU (samples, time_steps, features)

time_steps = 60  # using 60 previous days for each prediction

# Function to create sequences (sliding window)
def create_sequences(X, y, time_steps):
    X_seq, y_seq = [], []
    for i in range(time_steps, len(X)):
        X_seq.append(X.iloc[i-time_steps:i].values)  # Features from time_steps to current point
        y_seq.append(y.iloc[i])  # Target at current point
    return np.array(X_seq), np.array(y_seq)

# Create sequences for train and test data
X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps)
X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps)

# Print the reshaped data shape
print(f"Reshaped Training Set Shape: X_train_seq: {X_train_seq.shape}, y_train_seq: {y_train_seq.shape}")
print(f"Reshaped Testing Set Shape: X_test_seq: {X_test_seq.shape}, y_test_seq: {y_test_seq.shape}")

df.head()

df.isnull().sum()

#Check missing values
print("X_train NaNs:", X_train.isnull().sum().sum())
print("X_test NaNs:", X_test.isnull().sum().sum())

#Use forward fill first, then backward fill as a fallback
X_train = X_train.ffill().bfill()

#check again
print("X_train NaNs after fix:", X_train.isnull().sum().sum())  # should be 0

#Check missing values
print("y_train NaNs:", y_train.isnull().sum().sum())
print("y_test NaNs:", y_test.isnull().sum().sum())

print(X_train.columns)  # check if 'Close' is here by mistake

# To prevent target leakage;

# Full feature set (including lag features)
X_all = df.drop(columns=['Close'])
y_all = df['Close']

# For Linear Regression (no lag features to avoid leakage)
X_lr = X_all.drop(columns=['Close_Lag_1', 'Close_Lag_5'])

# For Random Forest (use all features)
X_rf = X_all.copy()

# For LSTM / GRU (they need reshaped sequences, handled separately)
X_lstm = ...

# Linear Regression
X_train_lr = X_lr.iloc[:train_size]
X_test_lr = X_lr.iloc[train_size:]
y_train = y_all.iloc[:train_size]
y_test = y_all.iloc[train_size:]

# Random Forest
X_train_rf = X_rf.iloc[:train_size]
X_test_rf = X_rf.iloc[train_size:]

# Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the model
lr_model = LinearRegression()

# Train the model
lr_model.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test)

# Evaluate the model
mse_lr = mean_squared_error(y_test, y_pred_lr)
r2_lr = r2_score(y_test, y_pred_lr)

print(f"Linear Regression MSE: {mse_lr}")
print(f"Linear Regression R2: {r2_lr}")

#Random Forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest MSE: {mse_rf}")
print(f"Random Forest R2: {r2_rf}")

# Combine feature and target into one DataFrame
X_train_lr['Close'] = y_train

# Drop all rows with any NaNs
X_train_lr_cleaned = X_train_lr.dropna()

# Separate back into X and y
y_train_clean = X_train_lr_cleaned['Close']
X_train_lr_clean = X_train_lr_cleaned.drop(columns=['Close'])

# Confirm no missing values
print("NaNs in X_train_lr_clean:", X_train_lr_clean.isnull().sum().sum())
print("NaNs in y_train_clean:", y_train_clean.isnull().sum())

print("NaNs in X_train_seq:", np.isnan(X_train_seq).sum())
print("NaNs in y_train_seq:", np.isnan(y_train_seq).sum())
print("NaNs in X_test_seq:", np.isnan(X_test_seq).sum())
print("NaNs in y_test_seq:", np.isnan(y_test_seq).sum())

# Create a mask for sequences that have no NaNs
valid_seq_mask = ~np.isnan(X_train_seq).any(axis=(1, 2))

# Filter out those sequences
X_train_seq = X_train_seq[valid_seq_mask]
y_train_seq = y_train_seq[valid_seq_mask]

print(" Cleaned X_train_seq NaNs:", np.isnan(X_train_seq).sum())

print("NaNs in X_train_seq:", np.isnan(X_train_seq).sum())
print("NaNs in y_train_seq:", np.isnan(y_train_seq).sum())
print("NaNs in X_test_seq:", np.isnan(X_test_seq).sum())
print("NaNs in y_test_seq:", np.isnan(y_test_seq).sum())

#LSTM
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Define the LSTM model
lstm_model = Sequential([
    LSTM(units=50, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    Dropout(0.2),
    LSTM(units=50, return_sequences=False),
    Dropout(0.2),
    Dense(units=25, activation='relu'),
    Dense(units=1)
])

# Compile the model
lstm_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
lstm_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32)

# Make predictions
y_pred_lstm = lstm_model.predict(X_test_seq)

# Evaluate the model (using RMSE)
rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_lstm))
print(f"LSTM RMSE: {rmse_lstm}")

#GRU
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout

# Define the GRU model
gru_model = Sequential([
    GRU(units=50, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    Dropout(0.2),
    GRU(units=50, return_sequences=False),
    Dropout(0.2),
    Dense(units=25, activation='relu'),
    Dense(units=1)
])

# Compile the model
gru_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
gru_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32)

# Make predictions
y_pred_gru = gru_model.predict(X_test_seq)

# Evaluate the model (using RMSE)
rmse_gru = np.sqrt(mean_squared_error(y_test_seq, y_pred_gru))
print(f"GRU RMSE: {rmse_gru}")

whos

subseq_count = 4
subseq_length = 15  # 60 // 4 = 15
features = X_train_seq.shape[2]

# Reshape training and testing sets
X_train_cnnlstm = X_train_seq.reshape((X_train_seq.shape[0], subseq_count, subseq_length, features))
X_test_cnnlstm = X_test_seq.reshape((X_test_seq.shape[0], subseq_count, subseq_length, features))

# CNN-LSTM
# Build the model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import TimeDistributed, Conv1D, MaxPooling1D, Flatten, LSTM, Dropout, Dense

cnn_lstm_model = Sequential([
    TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'),
                    input_shape=(subseq_count, subseq_length, features)),
    TimeDistributed(MaxPooling1D(pool_size=2)),
    TimeDistributed(Flatten()),
    LSTM(50),
    Dropout(0.2),
    Dense(25, activation='relu'),
    Dense(1)
])

cnn_lstm_model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
cnn_lstm_model.fit(X_train_cnnlstm, y_train_seq, epochs=10, batch_size=32)

# Evaluate the model
y_pred_cnnlstm = cnn_lstm_model.predict(X_test_cnnlstm)

from sklearn.metrics import mean_squared_error
import numpy as np

rmse_cnnlstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_cnnlstm))
print(f"CNN-LSTM RMSE: {rmse_cnnlstm}")





