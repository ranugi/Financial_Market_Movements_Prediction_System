# -*- coding: utf-8 -*-
"""beforeFE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lZUvMaYp3MTzher-rlcDB8eRp4Y2HDIP
"""

# Importing libraries for Data Manipulation
import pandas as pd
import numpy as np

# Importing libraries for Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Importing libraries for Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Importing libraries for Deep Learning
import torch
import torch.nn as nn
import torch.optim as optim

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Colab_Notebooks/NASDAQ_Historical_Prices.csv'

df = pd.read_csv(file_path)

df.head()

# Check for missing values in the dataset
missing_values = df.isnull().sum()
print(missing_values)

# Iterate through each column and check for duplicates
for column in df.columns:
    duplicates_in_column = df[column].duplicated()

    # Display duplicates in each column
    if duplicates_in_column.sum() > 0:
        print(f"Duplicates in column '{column}':")
        print(df[duplicates_in_column][[column]])
        print(f"Number of duplicate values in '{column}': {duplicates_in_column.sum()}")
        print("\n")
    else:
        print(f"No duplicates in column '{column}'.\n")

# Select only numeric columns
numeric_df = df.select_dtypes(include=[np.number])

# Calculate Q1 (25th percentile) and Q3 (75th percentile) for numeric columns
Q1 = numeric_df.quantile(0.25)
Q3 = numeric_df.quantile(0.75)

# Calculate the IQR (Interquartile Range)
IQR = Q3 - Q1

# Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Print the lower and upper bounds
print("Lower Bound for Outliers:")
print(lower_bound)
print("\nUpper Bound for Outliers:")
print(upper_bound)

# Identify outliers by comparing values with the bounds
outliers_iqr = (numeric_df < lower_bound) | (numeric_df > upper_bound)

# Count the number of outliers for each column
outlier_count_iqr = outliers_iqr.sum()

# Print the total number of outliers for each column
print("\nOutliers detected using IQR method:")
print(outlier_count_iqr)

# Remove rows where 'Volume' is an outlier
df_cleaned = df[(df['Volume'] >= lower_bound['Volume']) & (df['Volume'] <= upper_bound['Volume'])]

# Verify the new shape after removing outliers
print(f"Original dataset shape: {df.shape}")
print(f"Cleaned dataset shape: {df_cleaned.shape}")

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Standardizing columns (Open, High, Low, Close, Adj Close)
scaler = StandardScaler()
df[['Open', 'High', 'Low', 'Close', 'Adj Close']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Adj Close']])

# Min-Max Scaling for Volume
min_max_scaler = MinMaxScaler()
df['Volume'] = min_max_scaler.fit_transform(df[['Volume']])

# Summary statistics for the dataset
df.describe()

# Drop the 'Date' column
df = df.drop(columns=['Date'])

# Define the split index (80% for training, 20% for testing)
split_index = int(len(df) * 0.8)

# Split the dataset
train_df = df.iloc[:split_index]
test_df = df.iloc[split_index:]

# Print the shapes of the train and test sets
print(f"Training Set Shape: {train_df.shape}")
print(f"Testing Set Shape: {test_df.shape}")

import pandas as pd

# Save the cleaned dataset
df.to_csv("cleaned_dataset.csv", index=False)

# Download the file
from google.colab import files
files.download("cleaned_data.csv")
