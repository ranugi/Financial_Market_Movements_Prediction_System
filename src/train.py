# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TeyfBNB05G7XG1jaMoQKyMiuaelX1kvn
"""

# Data
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor

# Deep learning with TensorFlow/Keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/My Drive/Colab_Notebooks/cleaned_data.csv'

df = pd.read_csv(file_path)

df.head()

# Check for missing values in the dataset
missing_values = df.isnull().sum()
print(missing_values)

# to predict the 'Close' price
X = df.drop(columns=['Close'])
y = df['Close']  # Target variable

# Split the data (80% for training, 20% for testing)
train_size = int(len(df) * 0.8)

# Training and testing data (for time series, do not shuffle)
X_train = X.iloc[:train_size]
y_train = y.iloc[:train_size]
X_test = X.iloc[train_size:]
y_test = y.iloc[train_size:]

# Print the shape of the split data
print(f"Training Set Shape: X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"Testing Set Shape: X_test: {X_test.shape}, y_test: {y_test.shape}")

time_steps = 60  # using 60 previous days for each prediction

# Function to create sequences (sliding window)
def create_sequences(X, y, time_steps):
    X_seq, y_seq = [], []
    for i in range(time_steps, len(X)):
        X_seq.append(X.iloc[i-time_steps:i].values)  # Features from time_steps to current point
        y_seq.append(y.iloc[i])  # Target at current point
    return np.array(X_seq), np.array(y_seq)

# Create sequences for train and test data
X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps)
X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps)

# Print the reshaped data shape
print(f"Reshaped Training Set Shape: X_train_seq: {X_train_seq.shape}, y_train_seq: {y_train_seq.shape}")
print(f"Reshaped Testing Set Shape: X_test_seq: {X_test_seq.shape}, y_test_seq: {y_test_seq.shape}")

df.head()

df.isnull().sum()

#Check missing values
print("X_train NaNs:", X_train.isnull().sum().sum())
print("X_test NaNs:", X_test.isnull().sum().sum())

print(X_train.columns)  # check if 'Close' is here by mistake

#define GRU based model architecture
ru_model = Sequential([
    # First GRU layer with return_sequences=True to feed into next recurrent layer
    GRU(50, return_sequences=True, input_shape=(60, 5)),
    Dropout(0.2),   # Dropout to prevent overfitting

    # Second GRU layer (no return_sequences since it's the last recurrent layer)
    GRU(50),
    Dropout(0.2),

    # Fully connected layers for regression output
    Dense(25, activation='relu'), # hidden layer
    Dense(1)   # Output layer (predicting a single continuous value)
])

# Compile the model with Adam optimizer and MSE loss (standard for regression)
gru_model.compile(optimizer='adam', loss='mean_squared_error')

print("X_train_seq shape:", X_train_seq.shape)
print("X_test_seq shape:", X_test_seq.shape)

print("Sample input shape:", X_train_seq[0].shape)

print("X_train_seq shape:", X_train_seq.shape)
print("Sample input:", X_train_seq[0].shape)

X_train_seq = X_train_seq.transpose(0, 2, 1)
X_test_seq = X_test_seq.transpose(0, 2, 1)

print("X_train_seq shape:", X_train_seq.shape)
print("Sample input shape:", X_train_seq[0].shape)

X_train_seq = X_train_seq.transpose(0, 2, 1)  # (samples, 60, 5)
X_test_seq = X_test_seq.transpose(0, 2, 1)

print("Fixed shape:", X_train_seq.shape)  # should show (2001, 60, 5)

#gru model
gru_model = Sequential([
    GRU(50, return_sequences=True, input_shape=(60, 5)),
    Dropout(0.2),
    GRU(50),
    Dropout(0.2),
    Dense(25, activation='relu'),
    Dense(1)
])

#Train the model
gru_model.compile(optimizer='adam', loss='mean_squared_error')

history = gru_model.fit(
    X_train_seq, y_train_seq,
    epochs=10,        # Number of training cycles
    batch_size=32,    # Number of samples per batch
    validation_split=0.2,     # Reserve 20% of training data for validation
    callbacks=[EarlyStopping(patience=3)],     # Stop training if val loss doesn't improve for 3 epochs
    verbose=1        # Print training progress
)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Make predictions
y_pred_gru = gru_model.predict(X_test_seq)

# Calculate evaluation metrics
mse_gru = mean_squared_error(y_test_seq, y_pred_gru)
mae_gru = mean_absolute_error(y_test_seq, y_pred_gru)
rmse_gru = np.sqrt(mse_gru)

# Print metrics
print("GRU MSE:", mse_gru)
print("GRU MAE:", mae_gru)
print("GRU RMSE:", rmse_gru)

# Store predictions in memory

# LSTM
mse_lstm = mean_squared_error(y_test_seq, y_pred_lstm)
mae_lstm = mean_absolute_error(y_test_seq, y_pred_lstm)
rmse_lstm = np.sqrt(mse_lstm)

# CNN-LSTM
mse_cnn = mean_squared_error(y_test_seq, y_pred_cnnlstm)
mae_cnn = mean_absolute_error(y_test_seq, y_pred_cnnlstm)
rmse_cnn = np.sqrt(mse_cnn)

# Predicted vs Actual Plot
import matplotlib.pyplot as plt

# Plot for GRU
plt.figure(figsize=(10, 4))
plt.plot(y_test_seq[:100], label='Actual')
plt.plot(y_pred_gru[:100], label='GRU Predicted')
plt.title('GRU - Predicted vs Actual')
plt.xlabel('Time Steps')
plt.ylabel('Scaled Close Price')
plt.legend()
plt.show()

# Plot for LSTM
plt.figure(figsize=(10, 4))
plt.plot(y_test_seq[:100], label='Actual')
plt.plot(y_pred_lstm[:100], label='LSTM Predicted')
plt.title('LSTM - Predicted vs Actual')
plt.xlabel('Time Steps')
plt.ylabel('Scaled Close Price')
plt.legend()
plt.show()

# Plot for CNN-LSTM
plt.figure(figsize=(10, 4))
plt.plot(y_test_seq[:100], label='Actual')
plt.plot(y_pred_cnnlstm[:100], label='CNN-LSTM Predicted')
plt.title('CNN-LSTM - Predicted vs Actual')
plt.xlabel('Time Steps')
plt.ylabel('Scaled Close Price')
plt.legend()
plt.show()

# Residual Distribution (Histograms of errors)
# GRU Residuals
plt.hist((y_test_seq.flatten() - y_pred_gru.flatten()), bins=50, color='gray')
plt.title("GRU - Residual Distribution")
plt.xlabel("Error")
plt.ylabel("Frequency")
plt.show()

# LSTM Residuals
plt.hist((y_test_seq.flatten() - y_pred_lstm.flatten()), bins=50, color='skyblue')
plt.title("LSTM - Residual Distribution")
plt.xlabel("Error")
plt.ylabel("Frequency")
plt.show()

# CNN-LSTM Residuals
plt.hist((y_test_seq.flatten() - y_pred_cnnlstm.flatten()), bins=50, color='salmon')
plt.title("CNN-LSTM - Residual Distribution")
plt.xlabel("Error")
plt.ylabel("Frequency")
plt.show()

#RMSE bar chart (Model comparison)
import seaborn as sns
import pandas as pd

# Fill in your actual RMSE values here
data = {
    'Model': ['Linear Regression', 'Random Forest', 'LSTM', 'GRU', 'CNN-LSTM'],
    'RMSE': [0.0, 0.00035, rmse_lstm, rmse_gru, rmse_cnn]
}

df_metrics = pd.DataFrame(data)

plt.figure(figsize=(8, 5))
sns.barplot(x='Model', y='RMSE', data=df_metrics)
plt.title('Model RMSE Comparison')
plt.ylabel('Root Mean Squared Error')
plt.xticks(rotation=30)
plt.show()

# save the model
gru_model.save("models/trained_model.h5")

import os
print("trained_model.h5 exists:", os.path.exists("models/trained_model.h5"))

# download the model
from google.colab import files
files.download("models/trained_model.h5") 

from google.colab import drive
drive.mount('/content/drive')

!cp -r models /content/drive/MyDrive/



