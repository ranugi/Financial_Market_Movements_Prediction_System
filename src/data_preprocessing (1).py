# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lZUvMaYp3MTzher-rlcDB8eRp4Y2HDIP
"""

# Importing libraries for Data Manipulation
import pandas as pd
import numpy as np

# Importing libraries for Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Importing libraries for Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Importing libraries for Deep Learning
import torch
import torch.nn as nn
import torch.optim as optim

from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Colab_Notebooks/NASDAQ_Historical_Prices.csv'

df = pd.read_csv(file_path)

df.head()

# Check for missing values in the dataset
missing_values = df.isnull().sum()
print(missing_values)

# Iterate through each column and check for duplicates
for column in df.columns:
    duplicates_in_column = df[column].duplicated()

    # Display duplicates in each column
    if duplicates_in_column.sum() > 0:
        print(f"Duplicates in column '{column}':")
        print(df[duplicates_in_column][[column]])
        print(f"Number of duplicate values in '{column}': {duplicates_in_column.sum()}")
        print("\n")
    else:
        print(f"No duplicates in column '{column}'.\n")

from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Standardizing columns (Open, High, Low, Close, Adj Close)
scaler = StandardScaler()
df[['Open', 'High', 'Low', 'Close', 'Adj Close']] = scaler.fit_transform(df[['Open', 'High', 'Low', 'Close', 'Adj Close']])

# Min-Max Scaling for Volume
min_max_scaler = MinMaxScaler()
df['Volume'] = min_max_scaler.fit_transform(df[['Volume']])

# Summary statistics for the dataset
df.describe()

# Drop the 'Date' column
df = df.drop(columns=['Date'])

# Define the split index (80% for training, 20% for testing)
split_index = int(len(df) * 0.8)

# Split the dataset
train_df = df.iloc[:split_index]
test_df = df.iloc[split_index:]

# Print the shapes of the train and test sets
print(f"Training Set Shape: {train_df.shape}")
print(f"Testing Set Shape: {test_df.shape}")

import pandas as pd

# Save the cleaned dataset
df.to_csv("cleaned_data.csv", index=False)

# Download the file
from google.colab import files
files.download("cleaned_data.csv")